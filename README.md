ğŸ“Œ Simple-RAG-API

A lightweight Retrieval-Augmented Generation (RAG) API that runs locally with a local LLM (via Ollama). Built for simplicity and ease of experimentation â€” ideal for learning how to build vector-based search + generation without cloud APIs.

ğŸ§ª You get a minimal RAG backend that loads a local model, ingests text, and serves responses via HTTP in a Docker container â€” no cloud bills.

ğŸš€ Features

ğŸ§  Local LLM support using Ollama

ğŸ“Š Document embedding + retrieval pipeline

ğŸ› ï¸ Simple REST API powered by FastAPI (likely; adjust if you didnâ€™t use FastAPI)

ğŸ³ Docker support for easy deployment

ğŸ§ª Zero external dependencies (no OpenAI keys, no paid APIs)

ğŸ“¦ Clean folder structure (e.g., app.py, embed.py)

ğŸ“¥ Quick Start
ğŸ—ï¸ Clone it
git clone https://github.com/Sekiro4321/Simple-RAG-API.git
cd Simple-RAG-API

ğŸ³ Build the Docker image
docker build -t simple-rag-api .

ğŸš€ Run the API
docker run -p 8000:8000 simple-rag-api


Now your API should be live at http://localhost:8000.

ğŸ“¡ How It Works (High-Level)

Hereâ€™s the idea â€” no rocket science:

Embed incoming text using an embedding model in embed.py

Store vectors in memory (or a lightweight store)

Run a local LLM with Ollama to answer queries based on the nearest embeddings

Serve JSON responses through an HTTP API in app.py

This is basically a local RAG pipeline â€” simple, fast, and offline-friendly.

ğŸ“„ API Endpoints

ğŸ¯ Adjust these if your code uses different routes â€” just drop in the actual ones.

Method	Endpoint	Description
GET	/health	Check server status
POST	/embed	Submit text to create embeddings
POST	/query	Ask a question and get a RAG-powered reply
ğŸ› ï¸ Configuration

No external API keys needed!

Just ensure:

Ollama is installed and running locally

The local model is available for your container

Example .env (if used):

MODEL_NAME=your_local_model_here
PORT=8000


(If youâ€™re not using .env, keep configs in a config.py or similar.)

ğŸ“¦ Project Structure
Simple-RAG-API/
â”œâ”€â”€ app.py          # RAG API server code
â”œâ”€â”€ embed.py        # Embedding & vector logic
â”œâ”€â”€ Dockerfile      # Container image build
â”œâ”€â”€ k8s.txt         # Kubernetes example (optional)
â””â”€â”€ README.md       # This file

ğŸ§ª Development

Want to work on the code locally?

Create a Python virtual env:

python3 -m venv venv
source venv/bin/activate


Install deps:

pip install -r requirements.txt


Run locally:

uvicorn app:app --reload --host 0.0.0.0 --port 8000


Then test with curl or Postman. ğŸš€

ğŸ§  What This Is Great For

ğŸ“š Learning how RAG works end-to-end

ğŸ”’ Experimenting with local LLMs (no API bills)

ğŸ› ï¸ Building prototypes that donâ€™t depend on cloud

ğŸ§‘â€ğŸ’» Portfolio project to demo RAG basics
